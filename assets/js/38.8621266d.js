(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{451:function(t,a,r){"use strict";r.r(a);var s=r(3),_=Object(s.a)({},(function(){var t=this,a=t.$createElement,r=t._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("p",[t._v("这次在华迪做专业实习，一点都不水，还特硬核。")]),t._v(" "),r("h2",{attrs:{id:"_6-30"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_6-30"}},[t._v("#")]),t._v(" 6.30")]),t._v(" "),r("p",[t._v("上午：Python 基础，可参考"),r("a",{attrs:{href:"https://www.liaoxuefeng.com/wiki/1016959663602400",target:"_blank",rel:"noopener noreferrer"}},[t._v("廖雪峰博客"),r("OutboundLink")],1),t._v("。")]),t._v(" "),r("p",[t._v("下午：Python 的语法糖；"),r("a",{attrs:{href:"/python/scrapy"}},[t._v("Sracpy 爬虫")])]),t._v(" "),r("h2",{attrs:{id:"_7-1"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_7-1"}},[t._v("#")]),t._v(" 7.1")]),t._v(" "),r("p",[t._v("上午：爬虫和 Redis 进行结合，进行更复杂的爬虫（用爬到的链接再进行爬虫）。")]),t._v(" "),r("p",[t._v("下午：配环境："),r("a",{attrs:{href:"../hadoop-spark#%E5%AE%89%E8%A3%85"}},[t._v("Hadoop、Spark")]),t._v("。这里开始我转战 Linux 了，就没有跟着老师的教程走了，而是自己去找教程。一配配一天不是开玩笑的。")]),t._v(" "),r("h3",{attrs:{id:"_7-2"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_7-2"}},[t._v("#")]),t._v(" 7.2")]),t._v(" "),r("p",[t._v("上午："),r("a",{attrs:{href:"../hadoop-spark#hadoop-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86"}},[t._v("使用 HDFS + Spark 处理分布式数据库")])]),t._v(" "),r("p",[t._v("下午："),r("a",{attrs:{href:"/python/flask-data-visualization"}},[t._v("Flask 入门")]),t._v("。")]),t._v(" "),r("p",[t._v("知识就这么多，剩下的就是做项目了。")])])}),[],!1,null,null,null);a.default=_.exports}}]);