(window.webpackJsonp=window.webpackJsonp||[]).push([[88],{478:function(t,a,s){"use strict";s.r(a);var n=s(3),e=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("这里只用到了 Hadoop 的 Hadoop Distributed File System (HDFS)，即分布式文件系统。而数据处理是交给 Spark 了。")]),t._v(" "),a("h2",{attrs:{id:"安装"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#安装"}},[t._v("#")]),t._v(" 安装")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://wangchangchung.github.io/2017/09/28/Ubuntu-16-04%E4%B8%8A%E5%AE%89%E8%A3%85Hadoop%E5%B9%B6%E6%88%90%E5%8A%9F%E8%BF%90%E8%A1%8C/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hadoop"),a("OutboundLink")],1),t._v(" 和 "),a("a",{attrs:{href:"https://blog.csdn.net/lengconglin/article/details/77847623",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark"),a("OutboundLink")],1),t._v(" 在 Linux 下的安装方法")]),t._v(" "),a("p",[t._v("注意 Hadoop 的 namenode 默认管理 Web 页面是 "),a("code",[t._v("http://localhost:9870/")]),t._v("，而从 hdfs 协议访问 namenode 是从 9000 端口。")]),t._v(" "),a("h2",{attrs:{id:"hadoop-分布式文件管理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-分布式文件管理"}},[t._v("#")]),t._v(" Hadoop 分布式文件管理")]),t._v(" "),a("p",[t._v("管理 HDFS 的命令和 Linux 的命令很像。")]),t._v(" "),a("div",{staticClass:"language-sh extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token variable"}},[t._v("$HADOOP_HOME")]),t._v("\n./sbin/start-dfs.cmd        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 启动")]),t._v("\nhdfs dfs -mkdir /mydata     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建目录")]),t._v("\nhdfs dfs -put /path/to/file /mydata "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 上传")]),t._v("\nhdfs dfs -cat /mydata/file　"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 读取")]),t._v("\nhdfs dfs -rm -r /mydata     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删除")]),t._v("\n")])])]),a("p",[t._v("为配合后面的测试，将"),a("RouterLink",{attrs:{to:"/notes/python/scrapy.html"}},[t._v("爬虫")]),t._v("爬取的"),a("a",{attrs:{href:"ershoufang_price.txt"}},[t._v("二手价格数据")]),t._v("下载后，上传到 HDFS：")],1),t._v(" "),a("div",{staticClass:"language-sh extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token variable"}},[t._v("$HADOOP_HOME")]),t._v("\n./sbin/start-dfs.cmd\nhdfs dfs -mkdir /mydata\nhdfs dfs -put ershoufang_price.txt /mydata\n")])])]),a("p",[a("img",{attrs:{src:"/images/0016b9a25fdf18799477d80859dbeb74991e2a96a678680e2bea517deeab73f8.png",alt:"文件上传成功"}})]),t._v(" "),a("h2",{attrs:{id:"spark"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark"}},[t._v("#")]),t._v(" Spark")]),t._v(" "),a("p",[t._v("还需要安装 "),a("code",[t._v("pyspark")]),t._v("。")]),t._v(" "),a("div",{staticClass:"language-sh extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[t._v("pip3 "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" pyspark\n")])])]),a("p",[t._v("然后将环境变量写入 "),a("code",[t._v("/etc/profile")]),t._v("：")]),t._v(" "),a("div",{staticClass:"language-sh extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("export")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("PYSPARK_PYTHON")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("python3\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("export")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("PYSPARK_DRIVER_PYTHON")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("python3\n")])])]),a("p",[t._v("需要注销才能生效。")]),t._v(" "),a("p",[t._v("顺便一提，安装过程中还会自动安装 "),a("code",[t._v("py4j")]),t._v("，因为 Spark 是运行于 Java 之上，需要用 Python 读取 JVM 中的对象。")]),t._v(" "),a("h3",{attrs:{id:"读取文本文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#读取文本文件"}},[t._v("#")]),t._v(" 读取文本文件")]),t._v(" "),a("p",[t._v("尝试读取上面上传的 "),a("code",[t._v("ershoufang_price.txt")]),t._v("。这里的 "),a("code",[t._v("localhost:9000")]),t._v(" 可以在 Web 端看到。然后编写一下的 Python 代码，尝试从 HDFS 读取文本文件的内容。")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# SparkSession 操作 SparkSQL DataFrame，读取结构化数组")]),t._v("\nspark "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession \\\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("builder \\\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("appName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Spark SQL basic example"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#.config("spark.some.config.option", "some-value") \\')]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrCreate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# SparkContext 用于读取文本文件，读取分结构化数据")]),t._v("\nsc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sparkContext\n\nfilePath "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs://localhost:9000/mydata/ershoufang_price.txt"')]),t._v("\ntextFile "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filePath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回的 textFile 是 RDD (resilient distributed dataset)，是文件在 Spark 中的表示方法")]),t._v("\n\ndata "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collect"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# collect 是将所有 datanode 的数据收集整合到 namenode")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回的 data 是一个 list，其中每个元素对应 txt 的每一行")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("如果输出了 "),a("code",[t._v("44', '杭州,下沙,77,17382,44', '杭州,下沙,77,17382,44', '杭州,下沙,77,17382,44', '杭州,下沙,77,17382,44', '杭州,下沙,92,20924,43']")]),t._v(" 等，则正常。")]),t._v(" "),a("p",[t._v("如果报错："),a("code",[t._v("Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.")]),t._v("，则检查一下是否把环境变量加入 "),a("code",[t._v("/etc/profile")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"统计城市房源数并存入临时数据库"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#统计城市房源数并存入临时数据库"}},[t._v("#")]),t._v(" 统计城市房源数并存入临时数据库")]),t._v(" "),a("p",[t._v("接下来尝试将数据存到数据库。为测试语法，这里直接输出到 Spark SQL 的全局临时表中，然后做一个简单的查询。")]),t._v(" "),a("p",[t._v("这里的代码是紧接着上面的 "),a("code",[t._v("textFile = sc.textFile(filePath)")]),t._v("。")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Row\n\nfilePath "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs://localhost:9000/mydata/ershoufang_price.txt"')]),t._v("\ntextFile "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filePath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回的 textFile 是 RDD (resilient distributed dataset)，是文件在 Spark 中的一个表示")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# data = textFile.collect() # collect 是将所有 datanode 的数据收集整合到 namenode")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# # 返回的 data 是一个 list，其中每个元素对应 txt 的每一行")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(data)")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将 item 变为 (北京,1)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("to_pair")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    item_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" item_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将 (北京,1)(北京,1)(成都,1) 整合为 (北京,2)(成都,1)")]),t._v("\nrdd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to_pair"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduceByKey"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将上述 rdd 转为 DataFrame，放进临时表")]),t._v("\nrowRdd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("city"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" count"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndataFrame "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowRdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createGlobalTempView"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'SELECT * FROM global_temp.city ORDER BY count desc LIMIT 5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("输出结果如下：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("+----+-----+\n|city|count|\n+----+-----+\n|高新| 5717|\n|江北| 5100|\n|渝中| 4920|\n|和平| 4770|\n|南岸| 4710|\n+----+-----+\n")])])]),a("p",[t._v("重新运行代码，并在 "),a("code",[t._v("dataFrame.createGlobalTempView('city')")]),t._v(" 一行打断点暂停。暂停以后可进入 "),a("code",[t._v("localhost:4040")]),t._v("，即 Spark 的管理页面。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/87f78df095c9155ebc6112ac7041df2337d02a44ab980b7b52fb5d7078ac927f.png",alt:"Spark 管理页面"}})]),t._v(" "),a("h3",{attrs:{id:"将数据存入-mysql-数据库"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#将数据存入-mysql-数据库"}},[t._v("#")]),t._v(" 将数据存入 MySQL 数据库")]),t._v(" "),a("p",[t._v("这里首先需要提前配好 MySQL 数据库。")]),t._v(" "),a("p",[t._v("然后为 Spark 下载额外的 jar 包，用以操作 MySQL 数据库。")]),t._v(" "),a("div",{staticClass:"language-sh extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[t._v("wget")]),t._v(" https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.20/mysql-connector-java-8.0.20.jar "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据数据库版本而定")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("mv")]),t._v(" mysql-connector-java-8.0.20.jar /usr/local/spark/jars\n")])])]),a("p",[t._v("接着上文的 "),a("code",[t._v("dataFrame = spark.createDataFrame(rowRdd)")]),t._v(" 写：")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[t._v("dataFrame "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowRdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dataFrame.createGlobalTempView('city')")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# spark.sql('SELECT * FROM global_temp.city ORDER BY count desc LIMIT 5').show()")]),t._v("\n\nMySQL_Conn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'jdbc:mysql://localhost:3306/test_db?serverTimezone=UTC'")]),t._v("\nconn_param "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'user'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'root'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'password'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lyh54333'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'driver'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'com.mysql.cj.jdbc.Driver'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\ndataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("jdbc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MySQL_Conn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city_count'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'overwrite'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" conn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])}),[],!1,null,null,null);a.default=e.exports}}]);