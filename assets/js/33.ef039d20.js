(window.webpackJsonp=window.webpackJsonp||[]).push([[33],{418:function(t,a,r){"use strict";r.r(a);var o=r(3),s=Object(o.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("这次在华迪做专业实习，一点都不水，还特硬核。")]),t._v(" "),a("h2",{attrs:{id:"_6-30"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-30"}},[t._v("#")]),t._v(" 6.30")]),t._v(" "),a("p",[t._v("上午：Python 基础，可参考"),a("a",{attrs:{href:"https://www.liaoxuefeng.com/wiki/1016959663602400",target:"_blank",rel:"noopener noreferrer"}},[t._v("廖雪峰博客"),a("OutboundLink")],1),t._v("。")]),t._v(" "),a("p",[t._v("下午：Python 的语法糖；"),a("RouterLink",{attrs:{to:"/notes/python/scrapy.html"}},[t._v("Sracpy 爬虫")])],1),t._v(" "),a("h2",{attrs:{id:"_7-1"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-1"}},[t._v("#")]),t._v(" 7.1")]),t._v(" "),a("p",[t._v("上午：爬虫和 Redis 进行结合，进行更复杂的爬虫（用爬到的链接再进行爬虫）。")]),t._v(" "),a("p",[t._v("下午：配环境："),a("RouterLink",{attrs:{to:"/notes/java/hadoop-spark.html#安装"}},[t._v("Hadoop、Spark")]),t._v("。这里开始我转战 Linux 了，就没有跟着老师的教程走了，而是自己去找教程。一配配一天不是开玩笑的。")],1),t._v(" "),a("h3",{attrs:{id:"_7-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-2"}},[t._v("#")]),t._v(" 7.2")]),t._v(" "),a("p",[t._v("上午："),a("RouterLink",{attrs:{to:"/notes/java/hadoop-spark.html#hadoop-分布式文件管理"}},[t._v("使用 HDFS + Spark 处理分布式数据库")])],1),t._v(" "),a("p",[t._v("下午：Flask 入门。")]),t._v(" "),a("p",[t._v("知识就这么多，剩下的就是做项目了。")])])}),[],!1,null,null,null);a.default=s.exports}}]);