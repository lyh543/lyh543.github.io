(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{440:function(t,a,s){"use strict";s.r(a);var n=s(3),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("这里只用到了 Hadoop 的 Hadoop Distributed File System (HDFS)，即分布式文件系统。而数据处理是交给 Spark 了。")]),t._v(" "),s("h2",{attrs:{id:"安装"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#安装"}},[t._v("#")]),t._v(" 安装")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://wangchangchung.github.io/2017/09/28/Ubuntu-16-04%E4%B8%8A%E5%AE%89%E8%A3%85Hadoop%E5%B9%B6%E6%88%90%E5%8A%9F%E8%BF%90%E8%A1%8C/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hadoop"),s("OutboundLink")],1),t._v(" 和 "),s("a",{attrs:{href:"https://blog.csdn.net/lengconglin/article/details/77847623",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark"),s("OutboundLink")],1),t._v(" 在 Linux 下的安装方法")]),t._v(" "),s("p",[t._v("注意 Hadoop 的 namenode 默认管理 Web 页面是 "),s("code",[t._v("http://localhost:9870/")]),t._v("，而从 hdfs 协议访问 namenode 是从 9000 端口。")]),t._v(" "),s("h2",{attrs:{id:"hadoop-分布式文件管理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-分布式文件管理"}},[t._v("#")]),t._v(" Hadoop 分布式文件管理")]),t._v(" "),s("p",[t._v("管理 HDFS 的命令和 Linux 的命令很像。")]),t._v(" "),s("div",{staticClass:"language-sh extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sh"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token variable"}},[t._v("$HADOOP_HOME")]),t._v("\n./sbin/start-dfs.cmd        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 启动")]),t._v("\nhdfs dfs -mkdir /mydata     "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建目录")]),t._v("\nhdfs dfs -put /path/to/file /mydata "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 上传")]),t._v("\nhdfs dfs -cat /mydata/file　"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 读取")]),t._v("\nhdfs dfs -rm -r /mydata     "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删除")]),t._v("\n")])])]),s("p",[t._v("为配合后面的测试，将"),s("RouterLink",{attrs:{to:"/notes/python/scrapy.html"}},[t._v("爬虫")]),t._v("爬取的"),s("a",{attrs:{href:"ershoufang_price.txt"}},[t._v("二手价格数据")]),t._v("下载后，上传到 HDFS：")],1),t._v(" "),s("div",{staticClass:"language-sh extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sh"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token variable"}},[t._v("$HADOOP_HOME")]),t._v("\n./sbin/start-dfs.cmd\nhdfs dfs -mkdir /mydata\nhdfs dfs -put ershoufang_price.txt /mydata\n")])])]),s("p",[s("img",{attrs:{src:"/images/0016b9a25fdf18799477d80859dbeb74991e2a96a678680e2bea517deeab73f8.png",alt:"文件上传成功"}})]),t._v(" "),s("h2",{attrs:{id:"spark"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#spark"}},[t._v("#")]),t._v(" Spark")]),t._v(" "),s("p",[t._v("还需要安装 "),s("code",[t._v("pyspark")]),t._v("。")]),t._v(" "),s("div",{staticClass:"language-sh extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sh"}},[s("code",[t._v("pip3 "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" pyspark\n")])])]),s("p",[t._v("然后将环境变量写入 "),s("code",[t._v("/etc/profile")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-sh extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sh"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("export")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("PYSPARK_PYTHON")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("python3\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("export")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("PYSPARK_DRIVER_PYTHON")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("python3\n")])])]),s("p",[t._v("需要注销才能生效。")]),t._v(" "),s("p",[t._v("顺便一提，安装过程中还会自动安装 "),s("code",[t._v("py4j")]),t._v("，因为 Spark 是运行于 Java 之上，需要用 Python 读取 JVM 中的对象。")]),t._v(" "),s("h3",{attrs:{id:"读取文本文件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#读取文本文件"}},[t._v("#")]),t._v(" 读取文本文件")]),t._v(" "),s("p",[t._v("尝试读取上面上传的 "),s("code",[t._v("ershoufang_price.txt")]),t._v("。这里的 "),s("code",[t._v("localhost:9000")]),t._v(" 可以在 Web 端看到。然后编写一下的 Python 代码，尝试从 HDFS 读取文本文件的内容。")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# SparkSession 操作 SparkSQL DataFrame，读取结构化数组")]),t._v("\nspark "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession \\\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("builder \\\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("appName"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Spark SQL basic example"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#.config("spark.some.config.option", "some-value") \\')]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrCreate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# SparkContext 用于读取文本文件，读取分结构化数据")]),t._v("\nsc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sparkContext\n\nfilePath "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs://localhost:9000/mydata/ershoufang_price.txt"')]),t._v("\ntextFile "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filePath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回的 textFile 是 RDD (resilient distributed dataset)，是文件在 Spark 中的表示方法")]),t._v("\n\ndata "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" textFile"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collect"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# collect 是将所有 datanode 的数据收集整合到 namenode")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回的 data 是一个 list，其中每个元素对应 txt 的每一行")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("如果输出了 "),s("code",[t._v("44', '杭州,下沙,77,17382,44', '杭州,下沙,77,17382,44', '杭州,下沙,77,17382,44', '杭州,下沙,77,17382,44', '杭州,下沙,92,20924,43']")]),t._v(" 等，则正常。")]),t._v(" "),s("p",[t._v("如果报错："),s("code",[t._v("Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.")]),t._v("，则检查一下是否把环境变量加入 "),s("code",[t._v("/etc/profile")]),t._v("。")]),t._v(" "),s("h3",{attrs:{id:"统计城市房源数并存入临时数据库"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#统计城市房源数并存入临时数据库"}},[t._v("#")]),t._v(" 统计城市房源数并存入临时数据库")]),t._v(" "),s("p",[t._v("接下来尝试将数据存到数据库。为测试语法，这里直接输出到 Spark SQL 的全局临时表中，然后做一个简单的查询。")]),t._v(" "),s("p",[t._v("这里的代码是紧接着上面的 "),s("code",[t._v("textFile = sc.textFile(filePath)")]),t._v("。")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Row\n\nfilePath "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs://localhost:9000/mydata/ershoufang_price.txt"')]),t._v("\ntextFile "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filePath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回的 textFile 是 RDD (resilient distributed dataset)，是文件在 Spark 中的一个表示")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# data = textFile.collect() # collect 是将所有 datanode 的数据收集整合到 namenode")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# # 返回的 data 是一个 list，其中每个元素对应 txt 的每一行")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(data)")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将 item 变为 (北京,1)")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("to_pair")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    item_list "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" item_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将 (北京,1)(北京,1)(成都,1) 整合为 (北京,2)(成都,1)")]),t._v("\nrdd "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" textFile"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to_pair"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduceByKey"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将上述 rdd 转为 DataFrame，放进临时表")]),t._v("\nrowRdd "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("city"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" count"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowRdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createGlobalTempView"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nspark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'SELECT * FROM global_temp.city ORDER BY count desc LIMIT 5'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("输出结果如下：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("+----+-----+\n|city|count|\n+----+-----+\n|高新| 5717|\n|江北| 5100|\n|渝中| 4920|\n|和平| 4770|\n|南岸| 4710|\n+----+-----+\n")])])]),s("p",[t._v("重新运行代码，并在 "),s("code",[t._v("dataFrame.createGlobalTempView('city')")]),t._v(" 一行打断点暂停。暂停以后可进入 "),s("code",[t._v("localhost:4040")]),t._v("，即 Spark 的管理页面。")]),t._v(" "),s("p",[s("img",{attrs:{src:"/images/87f78df095c9155ebc6112ac7041df2337d02a44ab980b7b52fb5d7078ac927f.png",alt:"Spark 管理页面"}})]),t._v(" "),s("h3",{attrs:{id:"将数据存入-mysql-数据库"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#将数据存入-mysql-数据库"}},[t._v("#")]),t._v(" 将数据存入 MySQL 数据库")]),t._v(" "),s("p",[t._v("这里首先需要提前配好 MySQL 数据库。")]),t._v(" "),s("p",[t._v("然后为 Spark 下载额外的 jar 包，用以操作 MySQL 数据库。")]),t._v(" "),s("div",{staticClass:"language-sh extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sh"}},[s("code",[s("span",{pre:!0,attrs:{class:"token function"}},[t._v("wget")]),t._v(" https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.20/mysql-connector-java-8.0.20.jar "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据数据库版本而定")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("mv")]),t._v(" mysql-connector-java-8.0.20.jar /usr/local/spark/jars\n")])])]),s("p",[t._v("接着上文的 "),s("code",[t._v("dataFrame = spark.createDataFrame(rowRdd)")]),t._v(" 写：")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[t._v("dataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowRdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dataFrame.createGlobalTempView('city')")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# spark.sql('SELECT * FROM global_temp.city ORDER BY count desc LIMIT 5').show()")]),t._v("\n\nMySQL_Conn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'jdbc:mysql://localhost:3306/test_db?serverTimezone=UTC'")]),t._v("\nconn_param "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'user'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'root'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'password'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lyh54333'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'driver'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'com.mysql.cj.jdbc.Driver'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\ndataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("jdbc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MySQL_Conn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city_count'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'overwrite'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" conn_param"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])}),[],!1,null,null,null);a.default=e.exports}}]);