(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{432:function(t,e,v){"use strict";v.r(e);var _=v(3),r=Object(_.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("这本书比较薄，涉及的面也不像标题那么广，而是比较偏向机器学习的基础架构。作为一个科普书读一读还是很好的。")]),t._v(" "),e("h2",{attrs:{id:"加速乘加-向量运算"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#加速乘加-向量运算"}},[t._v("#")]),t._v(" 加速乘加/向量运算")]),t._v(" "),e("p",[t._v("Intel和AMD等CPU厂商的思路是，在ALU中增加SIMD（Single Instruction Multiple Data，单指令多数据）运算单元，在一条指令中计算多个数据，来加速此类运算。")]),t._v(" "),e("p",[t._v("GPU Text Shader -> CUDA Core：并行执行大量的加减乘除基本运算指令")]),t._v(" "),e("p",[t._v("GPU Tensor Core：实现在一个周期内计算一定大小以内的矩阵乘加。")]),t._v(" "),e("p",[t._v("Google TPU：去掉了其它功能的计算单元，只能用于进行乘加运算（思路：普通的GPGPU考虑了太多矩阵算法和向量算法以外的计算与实现，对于以深度神经网络和图像模式匹配为代表的AI算法，这些功能实属浪费）；一层计算完也不需要将结果写回内存，而是写到下一级流水线（读写内存的时间足够100次运算，直接数量级提升）")]),t._v(" "),e("h2",{attrs:{id:"tensorflow"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#tensorflow"}},[t._v("#")]),t._v(" Tensorflow")]),t._v(" "),e("p",[t._v("使用 Tensorflow 并不是调包侠，只是避免了重复造轮子（线性回归拟合等）")]),t._v(" "),e("h2",{attrs:{id:"gpu"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gpu"}},[t._v("#")]),t._v(" GPU")]),t._v(" "),e("p",[t._v("（P34）HBM内存、GPC处理集群->TPC纹理处理集群->SM流处理器->象限->CUDA Core+Tensor Core\n一个 H100 有 144 个 SM，18432 个 CUDA Core，576 个 Tensor Core。")]),t._v(" "),e("p",[t._v("GPU 服务器硬件\n（P50）为了解决 CPU PCIe 总线不够使用的问题：将 显卡、SSD、网卡分组，同组共用一个 PCIe Switch 连接到 CPU。GPU 可以通过 PCIe Switch "),e("strong",[t._v("直接访问网卡（以实现跨服务器的 GPU 的互访）和 SSD")]),t._v("，无需经过 CPU。")]),t._v(" "),e("p",[t._v("NVLink 和 NVLink Switch：用于解决"),e("strong",[t._v("同服务器间两 GPU、多 GPU 的数据互联")]),t._v("。一个 A100 上有 6 个 NVLink 端口，互联时可以在 6 条 NVLink 总线上同时向另一个 GPU 传输数据。")]),t._v(" "),e("p",[t._v("PCIe 和 NVLink 受到电气性能方面的原因，无法实现跨服务器的 GPU 通信。")]),t._v(" "),e("h2",{attrs:{id:"分布式训练涉及到的-io-问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#分布式训练涉及到的-io-问题"}},[t._v("#")]),t._v(" 分布式训练涉及到的 IO 问题")]),t._v(" "),e("ol",[e("li",[t._v("GPU 对其它节点的 GPU 下发 GPU 指令")]),t._v(" "),e("li",[t._v("GPU 和 GPU 之间的交互")]),t._v(" "),e("li",[t._v("GPU 和本地存储设备的交互")]),t._v(" "),e("li",[t._v("GPU 和远端存储设备的交互")])]),t._v(" "),e("p",[t._v("GPU Direct 技术")]),t._v(" "),e("ol",[e("li",[t._v("GPU Direct Shared Memory，让 GPU 直接读内存，跳过 DMA、CPU 复制。")]),t._v(" "),e("li",[t._v("GPU Direct P2P，让 GPU 通过 PCIe 直接读另一块 GPU 的内存，跳过 CPU，但仍需要经过 PCIe 的 Root Complex。所以 2016 年出现了 NVLink。")]),t._v(" "),e("li",[t._v("GPU Direct RDMA，让 GPU 直接通过网卡发送数据包。")]),t._v(" "),e("li",[t._v("GPU Direct Storage，让 GPU 直接读取 NVMe SSD。（限定 NVMe 是因为 NVMe 使用 PCIe！）")])]),t._v(" "),e("h2",{attrs:{id:"不同类型的分布式存储"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#不同类型的分布式存储"}},[t._v("#")]),t._v(" 不同类型的分布式存储")]),t._v(" "),e("ul",[e("li",[t._v("块存储：虚拟的磁盘。可以用作系统盘。")]),t._v(" "),e("li",[t._v("对象存储（S3、Ceph、Swift、MinIO）：成本低，易平滑扩容，可管理冷热数据，Web 接口可以直接读取。普通数据的首选。")]),t._v(" "),e("li",[t._v("高性能存储（HDFS、Lustre）：通过牺牲写性能和读写延迟，靠全节点的分布式来实现大吞吐量的读。对数据分析和机器学习这类读多写少的场景很有优势。")])])])}),[],!1,null,null,null);e.default=r.exports}}]);